{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does your data look like? (I)\n",
    "Up until now you have focused on creating new features and dealing with issues in your data. Feature engineering can also be used to make the most out of the data that you already have and use it more effectively when creating machine learning models.\n",
    "Many algorithms may assume that your data is normally distributed, or at least that all your columns are on the same scale. This will often not be the case, e.g. one feature may be measured in thousands of dollars while another would be number of years. In this exercise, you will create plots to examine the distributions of some numeric columns in the so_survey_df DataFrame, stored in so_numeric_df.\n",
    "\n",
    "1. Generate a histogram of all columns in the so_numeric_df DataFrame.\n",
    "2. Generate box plots of the Age and Years Experience columns in the so_numeric_df DataFrame.\n",
    "3. Generate a box plot of the ConvertedSalary column in the so_numeric_df DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "so_numeric_df.hist()\n",
    "plt.show()\n",
    "\n",
    "# Create a boxplot of two columns\n",
    "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a boxplot of ConvertedSalary\n",
    "so_numeric_df[['ConvertedSalary']].boxplot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does your data look like? (II)\n",
    "In the previous exercise you looked at the distribution of individual columns. While this is a good start, a more detailed view of how different features interact with each other may be useful as this can impact your decision on what to transform and how.\n",
    "\n",
    "1. Import matplotlib's pyplot module as plt.\n",
    "2. Import seaborn as sns.\n",
    "3. Plot pairwise relationships in the so_numeric_df dataset.\n",
    "4. Show the plot.\n",
    "5. Print the summary statistics of the so_numeric_df DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot pairwise relationships\n",
    "sns.pairplot(so_numeric_df)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(so_numeric_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "As discussed in the video, in normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest.\n",
    "When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.)\n",
    "\n",
    "1. Import MinMaxScaler from sklearn's preprocessing module.\n",
    "2. Instantiate the MinMaxScaler() as MM_scaler.\n",
    "3. Fit the MinMaxScaler on the Age column of so_numeric_df.\n",
    "4. Transform the same column with the scaler you just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "MM_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit MM_scaler to the data\n",
    "MM_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "print(so_numeric_df[['Age_MM', 'Age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is.\n",
    "\n",
    "1. Import StandardScaler from sklearn's preprocessing module.\n",
    "2. Instantiate the StandardScaler() as SS_scaler.\n",
    "3. Fit the StandardScaler on the Age column of so_numeric_df.\n",
    "4. Transform the same column with the scaler you just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Instantiate StandardScaler\n",
    "SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit SS_scaler to the data\n",
    "SS_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "print(so_numeric_df[['Age_SS', 'Age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log transformation\n",
    "In the previous exercises you scaled the data linearly, which will not affect the data's shape. This works great if your data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population. In this exercise you will use a log transform on the ConvertedSalary column in the so_numeric_df DataFrame as it has a large amount of its data centered around the lower values, but contains very high values also. These distributions are said to have a long right tail.\n",
    "\n",
    "1. Import PowerTransformer from sklearn's preprocessing module.\n",
    "2. Instantiate the PowerTransformer() as pow_trans.\n",
    "3. Fit the PowerTransformer on the ConvertedSalary column of so_numeric_df.\n",
    "4. Transform the same column with the scaler you just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Instantiate PowerTransformer\n",
    "pow_trans = PowerTransformer()\n",
    "\n",
    "# Train the transform on the data\n",
    "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Apply the power transform to the data\n",
    "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Plot the data before and after the transformation\n",
    "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage based outlier removal\n",
    "One way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if you are concerned that the highest values in your dataset should be avoided. When using this approach, you must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset.\n",
    "\n",
    "1. Find the 95th quantile of the ConvertedSalary column.\n",
    "2. Trim the so_numeric_df DataFrame to retain all rows where ConvertedSalary is less than it's 95th quantile.\n",
    "3. Plot the histogram of so_numeric_df[['ConvertedSalary']].\n",
    "4. Plot the histogram of trimmed_df[['ConvertedSalary']].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 95th quantile\n",
    "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
    "\n",
    "# Trim the outliers\n",
    "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "\n",
    "# The original histogram\n",
    "so_numeric_df[['ConvertedSalary']].hist()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# The trimmed histogram\n",
    "trimmed_df[['ConvertedSalary']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical outlier removal\n",
    "While removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together.\n",
    "\n",
    "\n",
    "1. Calculate the standard deviation and mean of the ConvertedSalary column of so_numeric_df.\n",
    "2. Calculate the upper and lower bounds as three standard deviations away from the mean in both the directions.\n",
    "3. Trim the so_numeric_df DataFrame to retain all rows where ConvertedSalary is within the lower and upper bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and transforming new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and testing transformations (I)\n",
    "So far you have created scalers based on a column, and then applied the scaler to the same data that it was trained on. When creating machine learning models you will generally build your models on historic data (train set) and apply your model to new unseen data (test set). In these cases you will need to ensure that the same scaling is being applied to both the training and test data.\n",
    "To do this in practice you train the scaler on the train set, and keep the trained scaler to apply it to the test set. You should never retrain a scaler on the test set.\n",
    "\n",
    "For this exercise and the next, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets.\n",
    "\n",
    "1. Instantiate the StandardScaler() as SS_scaler.\n",
    "2. Fit the StandardScaler on the Age column.\n",
    "3. Transform the Age column in the test set (so_test_numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply a standard scaler to the data\n",
    "SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit the standard scaler to the data\n",
    "SS_scaler.fit(so_train_numeric[['Age']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "print(so_test_numeric[['Age', 'Age_ss']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and testing transformations (II)\n",
    "Similar to applying the same scaler to both your training and test sets, if you have removed outliers from the train set, you probably want to do the same on the test set as well. Once again you should ensure that you use the thresholds calculated only from the train set to remove outliers from the test set.\n",
    "\n",
    "Similar to the last exercise, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets.\n",
    "\n",
    "1. Calculate the standard deviation and mean of the ConvertedSalary column.\n",
    "2. Calculate the upper and lower bounds as three standard deviations away from the mean in both the directions.\n",
    "3. Trim the so_test_numeric DataFrame to retain all rows where ConvertedSalary is within the lower and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "\n",
    "cut_off = train_std * 3\n",
    "train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "\n",
    "# Trim the test DataFrame\n",
    "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
    "                             & (so_test_numeric['ConvertedSalary'] > train_lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
